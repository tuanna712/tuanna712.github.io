<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Data Preparation EDA- MLDS</title>
    
        <!-- Favicons -->
        <link href="assets/img/fav-icon.png" rel="icon">
        <link href="assets/img/fav-icon.png" rel="apple-touch-icon">
    
        <!-- Link to external CSS -->
        <link rel="stylesheet" href="assets/css/nav.css">
        <link rel="stylesheet" href="assets/css/styles.css">
        <link rel="stylesheet" href="assets/css/data-prep.css">
    
        <!-- Link to external JS -->
        <script src="assets/js/loadNav.js"></script>
        <script src="assets/js/loadCollectionActions.js"></script>
        <script src="https://kit.fontawesome.com/a076d05399.js" crossorigin="anonymous"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </head>

    <body>
        <!-- Menu Bar -->
        <div id="nav"></div>
        
        <!-- Main -->
        <div class="container">
            <h1 class="title">
                Clustering
            </h1>

            <p class="h2-content">
                Clustering is an unsupervised machine learning technique used to group similar data points into clusters based on 
                their inherent patterns and similarities. It aims to discover natural structures in data without predefined labels, 
                making it useful for tasks like customer segmentation, anomaly detection, and pattern recognition. General approaches 
                to clustering include partition-based methods (e.g., K-Means), which divide data into a fixed number of clusters; 
                hierarchical methods, which create nested groupings through iterative merging or splitting; density-based methods 
                (e.g., DBSCAN), which identify clusters based on dense regions while ignoring noise; and model-based methods 
                (e.g., Gaussian Mixture Models), which assume an underlying probabilistic distribution. The choice of method 
                depends on the data’s structure, cluster shapes, and the need for scalability or robustness to noise.
            </p>

            <section>
                <h2>Overview</h2>
                <p class="h2-content">
                    <b>Partition-Based Clustering (K-Means):</b> K-Means is a widely used clustering algorithm that partitions data into 
                    k clusters by minimizing intra-cluster variance. K-Means works by randomly initializing k centroids, assigning data 
                    points to the nearest centroid, and updating the centroids based on the mean of the assigned points until convergence.
                    <ul>
                        <li>It assumes clusters are spherical and of similar size, making it less effective for irregularly shaped clusters.</li>
                        <li>The number of clusters (k) must be predefined, which can be difficult if the optimal k is unknown. </li>
                        <li>Despite being sensitive to outliers and initial centroids, K-Means is computationally efficient, 
                            making it suitable for large datasets like customer segmentation and image compression.</li>
                    </ul>
                    <div class="image">
                        <img style="height: 320px; width: 700px;" src="https://miro.medium.com/v2/resize:fit:561/0*mQCGBdYhzZ8YMZPv.png" alt="clustering-data" class="grid-image" data-description="">
                    </div>
                    <p class="image-caption">KMeans Clustering.
                        (<a href="https://medium.com/analytics-vidhya/k-means-clustering-3ce2456db7f1" target="_blank">Source</a>)
                    </p>
                </p>
                <p class="h2-content">
                    <b>Hierarchical Clustering:</b> Hierarchical clustering builds a hierarchy of nested clusters using either an agglomerative 
                    (bottom-up) or divisive (top-down) approach. In the agglomerative method, each data point starts as an individual cluster,
                     and clusters are iteratively merged based on similarity until a single cluster remains. 
                     Some properties of Hierarchical Clustering are:
                     <ul>
                        <li>The divisive approach starts with all data points in one cluster and recursively splits them into smaller clusters. </li>
                        <li>The results are visualized in a dendrogram, which helps determine the optimal number of clusters without explicitly setting k beforehand. </li>
                        <li>Hierarchical clustering is effective for structured analysis.</li>
                        <li>It is computationally expensive, making it impractical for large datasets. </li>
                        <li>Its sensitivity to the choice of linkage method can affect cluster formation, sometimes leading to poor results when noise is present.</li>
                    </ul>
                    <div class="image">
                        <img style="height: 250px; width: 700px;" src="assets/img/clustering/hclust.png" alt="clustering-data" class="grid-image" data-description="">
                    </div>
                    <p class="image-caption">Hierarchical Clustering.
                        (<a href="https://www.researchgate.net/publication/342141592_A_Review_of_Super-Resolution_Single-Molecule_Localization_Microscopy_Cluster_Analysis_and_Quantification_Methods" target="_blank">Source</a>)
                    </p>
                </p>
                <p class="h2-content">
                    <b>Density-Based Clustering (DBSCAN):</b> DBSCAN identifies clusters based on data density, making it particularly useful 
                    for detecting arbitrarily shaped clusters and distinguishing noise. DBSCAN groups closely packed data points and treats sparse points as outliers. 
                    It relies on two parameters: \( \epsilon \) (epsilon) for the neighborhood radius and MinPts for the minimum points required to form a dense region.
                    <ul>
                        <li>Unlike K-Means, DBSCAN doesn't require specifying the number of clusters and can handle clusters with varying densities.</li>
                        <li>It may struggle when cluster densities vary significantly, as a single ε and MinPts setting might not work well for all data.</li>
                        <li>DBSCAN is commonly used in anomaly detection, geographic clustering, and noise filtering in large datasets.</li>
                    </ul>
                    <div class="image">
                        <img style="height: 220px; width: 700px;" src="assets/img/clustering/dbscan_example.png" alt="clustering-data" class="grid-image" data-description="">
                    </div>
                    <p class="image-caption">DBSCAN Clustering.
                        (<a href="https://www.kdnuggets.com/unveiling-hidden-patterns-an-introduction-to-hierarchical-clustering" target="_blank">Source</a>)
                    </p>
                </p>
                <p class="h2-content">
                    <b>In short,</b> all three methods are unsupervised clustering techniques that work with unlabeled data and provide insights based on the data's structure. 
                    Each method has its strengths and weaknesses.<br><br>
                    <b>Compare and Contrast: Advantages</b>
                    <ul>
                        <li>K-Means is fast and works well for large datasets.</li>
                        <li>Hierarchical Clustering doesn’t need you to specify the number of clusters, making it flexible for different kinds of analysis.</li>
                        <li>DBSCAN overcomes K-Means’ limitation by handling clusters of different shapes and densities better.</li>
                    </ul>
                    <b>Compare and Contrast: Limitations</b>
                    <ul>
                        <li>K-Means requires you to specify the number of clusters (k) in advance and groups data based on spherical distance.</li>
                        <li>Hierarchical Clustering depends on the chosen linkage method and can be slow with large datasets.</li>
                        <li>DBSCAN relies on two important parameters (epsilon and MinPts) and might miss some clusters if these parameters are not set correctly.</li>
                    </ul>
                </p>
                
                <p class="h2-content">
                    <b>Distance Measures</b><br>
                    In hierarchical clustering, a distance measure quantifies the similarity or dissimilarity between data points, directly shaping cluster formation. 
                    Since the method builds a dendrogram by iteratively merging or splitting clusters, the choice of distance metric influences which points are grouped together. 
                    
                    <div class="image">
                        <img style="height: 350px; width: 700px;" src="assets/img/clustering/distance.png" alt="clustering-data" class="grid-image" data-description="">
                    </div>
                    <p class="image-caption">Six types of distance measures.
                        (<a href="https://www.maartengrootendorst.com/blog/distances/" target="_blank">Source</a>)
                    </p>

                    Common measures like Euclidean, Manhattan, and Cosine similarity suit different data types. Selecting an appropriate metric ensures meaningful clustering 
                    that reflects the data’s inherent structure.
                    <ul>
                        <li>Manhattan Distance (or L1 norm) is calculated as:
                                \[
                                d_{Manhattan}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{n} |x_i - y_i|
                                \]
                        </li>
                        <li>
                            Euclidean Distance (or L2 norm): between two points \( \mathbf{x} = (x_1, x_2, \dots, x_n) \) and 
                                \( \mathbf{y} = (y_1, y_2, \dots, y_n) \) is given by:
                                \[
                                d_{Euclidean}(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
                                \]
                        </li>
                        <li>Cosine Similarity between two vectors is given by:
                                \[
                                \cos(\theta) = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \cdot \sqrt{\sum_{i=1}^{n} y_i^2}}
                                \]
                        </li>
                    </ul>
                </p>

                <p class="h2-content">
                    <b>How Clustering is used in this project?</b><br>
                    In this project, Kmeans, Hierarchical Clustering, and DBSCAN are applied to unsupervisedly cluster the dataset. 
                    The label column is removed, and the remaining features are normalized.
                    The K-Means algorithm is used to partition the data into clusters, while Hierarchical Clustering provides a dendrogram
                    to visualize the relationships between data points. DBSCAN is employed to identify clusters of varying shapes and densities.
                    Finally, the results of these clustering techniques are compared and analyzed to gain insights into the dataset's structure.
                </p>
            </section>

            <h2>Implementation</h2>
            <section style="margin-left: 33px;">

<!-- 1. Data Preparation for Clustering -->
                <h3>1. Data Preparation for Clustering</h3>
                <p class="h2-content">
                    After data preprocessing process, we have a dataset which has been removed null values and cleaned outliers.
                    The dataset contains 17 features. The features are a mix of numerical and categorical variables.<br> 
                    However, K-Means and other clustering algorithms only work with numerical features. In this step, label column and categorical features are removed.
                    The remaining features are then normalized using StandardScaler. The image below shows the data screenshot before and after removal and normalization.
                    <div class="image">
                        <img style="height: 550px;" src="assets/img/clustering/drop-label-data.png" alt="clustering-data" class="grid-image" data-description="">
                    </div>
                    <p class="image-caption">Data before and after preprocessing.</p>

                    The historam of all 13 features after normalization is shown below.

                    <div class="image">
                        <img style="height: 350px;" src="assets/img/clustering/features_hist.png" alt="pca-cleaned-data" class="grid-image" data-description="">
                    </div>
                    <p class="image-caption">Histogram of all features after normalization.</p>
                    
                    All the datasets and preprocessing code can be found in the links below:
                    <div class="external-links" style="margin-top: 10px;">
                        <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/pca/cleaned_data.csv" target="_blank" class="item">
                            <img src="assets/icons/table-solid.svg" alt="dataset" data-description="">
                            Cleaned Dataset
                        </a>
                        <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/clustering/cleaned_data_nolabel.csv" target="_blank" class="item">
                            <img src="assets/icons/table-solid.svg" alt="dataset" data-description="">
                            Dropped Label Dataset
                        </a>
                        <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/clustering/normalized_data.csv" target="_blank" class="item">
                            <img src="assets/icons/table-solid.svg" alt="dataset" data-description="">
                            Normalized Dataset
                        </a>
                        <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/04_clustering.ipynb" target="_blank" class="item">
                            <img src="assets/icons/github-brands-solid.svg" alt="github" data-description="">
                            Data Processing Code
                        </a>
                    </div>
                </p>
                <p class="h2-content">
                    <b>Principal Component Analysis: </b><br>
                    To prepare data for the next stage, a PCA with n_components=3 is applied to the dataset.
                    The dimenstionality is reduced from 13 to 3 and covers 55% of the variance/information of the data.
                    The image below shows the PCA result.
                    <div class="image">
                        <img style="height: 280px;" src="assets/img/clustering/pca_3_clustering.png" alt="pca-cleaned-data" class="grid-image" data-description="">
                    </div>
                    <p class="image-caption">Dataset after applying PCA with 3 components and the corresponding bar chart of cumulative explained variance ratio.</p>
                    
                    <div class="image">
                        <img style="height: 400px;" src="assets/img/clustering/pca_3d.gif" alt="pca-cleaned-data" class="grid-image" data-description="">
                    </div>
                    <p class="image-caption">3D visualization of PCA-transformed data, with points colored by lithology types. 
                        The distribution suggests the potential for clustering using unsupervised learning techniques.
                    </p>
                    The PCA data (n_components=3) can be found in the links below:
                    <div class="external-links" style="margin-top: 10px;">
                        <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/clustering/pca_data.csv" target="_blank" class="item">
                            <img src="assets/icons/table-solid.svg" alt="dataset" data-description="">
                            PCA (3PCs) Dataset
                        </a>
                    </div>
                </p>

<!-- 2. K-Means Clustering -->
                <h3>2. K-Means Clustering</h3>
                <p class="h2-content">
                    The K-Means algorithm requires defining the number of clusters (\(k\)) beforehand, 
                    but choosing the optimal k is a common challenge in clustering analysis. 
                    Several methods, such as the Elbow method, Silhouette score, and Gap statistic, can help identify the best k value. 

                    <div class="image">
                        <img style="height: 200px; width: 600px;" src="assets/img/clustering/silhouette.png" alt="silhouette" class="grid-image" data-description="">
                    </div>
                    <p class="image-caption">Silhouette Analysis on input value to find out top 3 optinal k values.</p>

                    In this case, the <b>Silhouette</b> score analysis suggests that the top three <b>optimal k values are  \(4, 3\) and \(7\). </b>
                    Details of Silhouette analysis can be found in the code link at the end of this section.<br><br>

                    The K-Means algorithm is then applied to the PCA 3-Components dataset with \(k = 4, 3\) and \(7\).
                    The images below show the clustering results with \(k = 4, 3\) and \(7\). The centroids are marked with black crosses.
                    The data points are colored in both cluster colors and lithology labels, which supports the clustering interpretation.
                    Click on the image to open expanded view mode.
                </p>
                <div class="image-grid">
                    <!-- Importing images... -->
                    <img src="assets/img/clustering/kmean_4.png" alt="kmean-4" class="grid-image" data-description="">
                    <img src="assets/img/clustering/kmean_3.png" alt="kmean-3" class="grid-image" data-description="">
                    <img src="assets/img/clustering/kmean_7.png" alt="kmean-7" class="grid-image" data-description="">
                </div>
                <p class="image-caption">Scatter visualization of K-Means Clustering (\(k=4, 3, 7\)) using PC0 and PC1 axies. <br>
                    The data points are colored by clusters (upper) and lithology labels (lower).</p>
                <p class="h2-content">
                    The figures above show that all three K-Means clustering results group most of the Sandstone data points into a single cluster. 
                    However, some Sandstone values are also scattered across other clusters. With \(k=4\) and \(k=7\), the algorithm successfully identifies 
                    Chalk (yellow) as a separate cluster. Overall, the clustering performance is not ideal, likely due to the original data points 
                    not being well-separated in a spherical manner, which affects the effectiveness of the K-Means algorithm.
                    <br>
                    The code of Silhouette analysis and K-Means clustering can be found in the link below:
                    <div class="external-links" style="margin-top: 10px;">
                        <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/04_clustering.ipynb" target="_blank" class="item">
                            <img src="assets/icons/github-brands-solid.svg" alt="github" data-description="">
                            Silhouette analysis and K-Means clustering
                        </a>
                    </div>
                </p>
<!-- 3. Hierarchical Clustering -->
                <h3>3. Hierarchical Clustering & Cosine Similarity</h3>
                To implement Hierarchical Clustering, the distance measure is set to calculate the distance between data points. 
                Euclidean distance measures the absolute difference between points, forming clusters based on physical proximity, making it suitable 
                for compact, spherical clusters. 
                In contrast, cosine similarity captures the angle between vectors, grouping items with similar patterns regardless of magnitude, 
                making it ideal for text and high-dimensional sparse data. Euclidean is sensitive to scale, often paired with Ward’s method, 
                while cosine similarity is robust to scale differences and works well with average or complete linkage.<br>
                The default distance measure is Euclidean distance, but in this case, Cosine Similarity is also used to compare the clustering results.
                <div class="image">
                    <img style="height: 300px;" src="assets/img/clustering/hclust_dendrogram.png" alt="pca-cleaned-data" class="grid-image" data-description="">
                </div>
                <p class="image-caption">Dendrogram for Hierarchical Clustering with Euclidean and Cosine Similarity</p>

                The dendrogram above shows the clustering results of Hierarchical Clustering with Euclidean and Cosine Similarity distance measures.
                The y-axis represents the distance between clusters, while the x-axis shows the data points. At glance, the clustering with cosine similarity
                seems to be more effective than the one with Euclidean distance. The distance are significant, and the clusters are well-separated.

                <div class="image">
                    <img style="height: 350px;" src="assets/img/clustering/hclust_comparison.png" alt="pca-cleaned-data" class="grid-image" data-description="">
                </div>
                <p class="image-caption">Hierarchical Clustering Results compared to the labels</p>

                The image above shows the clustering results of Hierarchical Clustering with Cosine Similarity distance measure. The result points out the same number of clusters
                as the label values. However, it can't separate the clusters well. The hierarchical clustering captures the general trend of the data but fails to identify the specific clusters.
                For example, cluster 5 contains 2 labels (Chalk and Limestone), cluster 1 covers a scatter of data points. While sandstone is discretized into 3 clusters.<br><br>
                The code for this part can be found in the Deriverables section.

<!-- 4. Density-Based Clustering (DBSCAN) -->
                <h3>4. Density-Based Clustering (DBSCAN)</h3>
                To implement DBSCAN, the hyperparameters epsilon and MinPts are experimented with different values to find the optimal. In this section, 3 sets of parameters are used, including
                {epsilon=0.3, MinPts=30}, {epsilon=0.4, MinPts=50} and {epsilon=0.2, MinPts=7}. The results are shown in the images below.
                <div class="image-grid">
                    <!-- Importing images... -->
                    <img src="assets/img/clustering/dbscan_1.png" alt="dbscan_1" class="grid-image" data-description="">
                    <img src="assets/img/clustering/dbscan_2.png" alt="dbscan_2" class="grid-image" data-description="">
                    <img src="assets/img/clustering/dbscan_3.png" alt="dbscan_3" class="grid-image" data-description="">
                </div>
                <p class="image-caption">2D Visualization of DBSCAN's results on different principal components.</p>
                All three models partially captured the largest label (Sandstone) along with Limestone and SandstoneShale. 
                However, these labels do not form spherical shapes, making them difficult to cluster using the K-Means algorithm. 
                In contrast, DBSCAN, which can identify clusters of varying shapes and densities, proved more effective for this dataset. 
                However, smaller and more scattered labels were not well captured, posing challenges for all unsupervised clustering algorithms.
                <div class="image">
                    <img style="height: 500px;" src="assets/img/clustering/dbscan_3d.gif" alt="pca-cleaned-data" class="grid-image" data-description="">
                </div>
                <p class="image-caption">3D Visualization of the result of DBSCAN Clustering</p>
                The 3D visualization of DBSCAN clustering shows that the algorithm can identify clusters of varying shapes and densities.<br><br>
                The code for this part can be found in the Deriverables section.
            </section>
<!-- Conclusion: Related to the topic(non-technical). -->
            <section>
                <h2>Conclusion</h2>
                <p class="h2-content">
                    Kmean results followed the spherical distance, which only captured the main part of <b>Sandstone</b> and had poor performance on other labels.
                    Hierarchical Clustering with Cosine Similarity distance doesn't show a better performance than Kmeans, but it can capture the general trend of the data (<b>Sandstone, Chalk and Limestone</b>).
                    DBSCAN is the best clustering algorithm for this dataset. It can identify clusters of varying shapes and densities include some lithology labels (<b>Sandstone along with Limestone and SandstoneShale</b>).
                    In conclusion, all applied three clustering algorithms are unsupervised learning techniques that work with unlabeled data and provide insights based on the data's structure.
                    However, it is still limited by the original data points not being well-separated and raise the challenge to all clustering algorithms.
                </p>
            </section>
<!-- Deliverables -->
            <section>
                <h2>Deliverables</h2>
                <p class="h2-content">
                    For your reference, all external links are provided below:
                </p>
                <div class="external-links">
                    <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/pca/cleaned_data.csv" target="_blank" class="item">
                        <img src="assets/icons/table-solid.svg" alt="data" data-description="">
                        Cleaned Data
                    </a>
                    <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/clustering/cleaned_data_nolabel.csv" target="_blank" class="item">
                        <img src="assets/icons/table-solid.svg" alt="data" data-description="">
                        Dropped Label Data
                    </a>
                    <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/clustering/normalized_data.csv" target="_blank" class="item">
                        <img src="assets/icons/table-solid.svg" alt="data" data-description="">
                        Normalized Data
                    </a>
                    <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/clustering/pca_data.csv" target="_blank" class="item">
                        <img src="assets/icons/table-solid.svg" alt="data" data-description="">
                        PCA (3PCs) Data
                    </a>
                    <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/04_clustering.ipynb" target="_blank" class="item">
                        <img src="assets/icons/github-brands-solid.svg" alt="github" data-description="">
                        Clustering Code
                    </a>
                </div>
            </section>
        </div>
        <!-- Image Expanded View -->
        <div class="overlay" id="overlay">
            <div>
                <button class="close-btn" id="closeBtn"></button>
            </div>
            <div class="img-disp" id="imgDisp">
                <img id="expandedImage" src="" alt="Expanded Image">
            </div>

            <div class="img-desc" id="imgDesc">
                <p id="imageDescription"></p>
            </div>
        </div>
        <!-- Footer -->
        <footer>
            <div class="footer-container">
                <p>&copy; 2025 Tuan Nguyen.</p>
            </div>
        </footer>
    </body>
</html>
