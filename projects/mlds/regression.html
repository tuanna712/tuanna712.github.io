<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Data Preparation EDA- MLDS</title>
    
        <!-- Favicons -->
        <link href="assets/img/fav-icon.png" rel="icon">
        <link href="assets/img/fav-icon.png" rel="apple-touch-icon">
    
        <!-- Link to external CSS -->
        <link rel="stylesheet" href="assets/css/nav.css">
        <link rel="stylesheet" href="assets/css/styles.css">
        <link rel="stylesheet" href="assets/css/data-prep.css">
    
        <!-- Link to external JS -->
        <script src="assets/js/loadNav.js"></script>
        <script src="assets/js/loadCollectionActions.js"></script>
        <script src="https://kit.fontawesome.com/a076d05399.js" crossorigin="anonymous"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    </head>

    <body>
        <!-- Menu Bar -->
        <div id="nav"></div>
        
        <!-- Main -->
        <div class="container">
            <h1 class="title">
                Regression
            </h1>

        <!-- Overview -->
            <h2>Overview</h2>

            <p class="h2-content">
                <ol>
                    <li>
                        <p class="h2-content">
                            <b>Define and explain linear regression?</b><br>
                            Linear Regression is a statistical method used to model the relationship between a dependent variable 
                            and one or more independent variables. It assumes a straight-line relationship and is expressed as:
                            <p style="display: flex; justify-content: center; align-items: center; height: auto; text-align: center;">
                                \( 
                                    \begin{align*}
                                    Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n + \epsilon
                                    \end{align*}
                                \)
                            </p>
                            Where:
                                <ul>
                                    <li>\( Y \) is the dependent variable (target)</li>
                                    <li>\( X_1, X_2, ..., X_n \) are the independent variables (features)</li>
                                    <li>\( \beta_0, \beta_1, ..., \beta_n \) are the coefficients/weights</li>
                                    <li>\( \epsilon \) is the error term (residual)</li>
                                </ul>

                            <b><em>How the model is trained:</em></b>
                            <ul>
                                <li>Linear regression is trained by minimizing the <i>Mean Squared Error (MSE)</i>:
                                    <p style="display: flex; justify-content: center; align-items: center; height: auto; text-align: center;">
                                        \( \text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 \)
                                    </p>
                                </li>
                                <li>
                                    The optimal coefficients \( \beta \) are estimated using methods like 
                                    Ordinary Least Squares (OLS) or Gradient Descent.
                                </li>
                            </ul>
                            <b><em>How it makes predictions:</em></b>
                            <ul>
                                <li>
                                    Once trained, the model predicts outcomes by plugging input feature values into the learned linear equation.
                                </li>
                            </ul>
                        </p>
                    </li>

                    <li>
                        <p class="h2-content">
                            <b>Define and explain logistic regression?</b><br>
                            Logistic Regression is used for classification problems where the output is categorical (e.g., 0 or 1, True or False).
                            Instead of fitting a straight line, it uses the Sigmoid function to output probabilities between 0 and 1:
                            <p style="display: flex; justify-content: center; align-items: center; height: auto; text-align: center;">
                                \(
                                    P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
                                    \)
                            </p>
                            <div class="image">
                                <img style="height: 450px; " src="assets/img/reg/logistic_example.png" alt="Logistic Regression" class="grid-image" data-description="">
                                <p class="image-caption">Example of Logistic Regression.(<a href="https://doi.org/10.1371/journal.pone.0290762" target="_blank">Source</a>)</p>
                            </div>
                            <p class="h2-content">
                                <b><em>How the model is trained:</em></b>
                                <ul>
                                        <li>
                                            Logistic regression is trained using <i>Maximum Likelihood Estimation (MLE)</i>, aiming to find the coefficients that maximize
                                            the likelihood of observing the given outcomes. 
                                        </li> 
                                        <li>
                                            The loss function used is <i>Binary Cross-Entropy</i>:<br>
                                            <p style="display: flex; justify-content: center; align-items: center; height: auto; text-align: center;">
                                                \( \text{Loss} = -\frac{1}{n} \sum_{i=1}^n [y_i \log(\hat{y}_i) + (1 - y_i)\log(1 - \hat{y}_i)] \)
                                            </p>
                                        </li>
                                    </ul>
                            </p>
                            <p  class="h2-content">
                                <b><em>How it makes predictions:</em></b>
                                <ul>
                                    <li>The model computes the probability of the output belonging to class 1.</li>
                                    <li>If the predicted probability exceeds a threshold (e.g., 0.5), the prediction is class 1; otherwise, it is class 0.</li>
                                    <li>This allows logistic regression to separate data into different classes based on probability thresholds.</li>
                                </ul>
                            </p>
                        </p>
                    </li>

                    <li>
                        <p class="h2-content">
                        <b>How are they similar and how are they different?</b><br>
                            <b><em>Similarities:</em></b> 
                            <ul>
                                <li>Both linear and logistic regression analyze relationships between independent and dependent variables 
                            and use coefficients (<code>β</code>) to determine their impact.</li>
                            </ul>
                            <b><em>Differences:</em></b> 
                            <ul>
                                <li>Linear regression predicts continuous values, while logistic regression predicts probabilities for classification.</li>
                                <li>Linear regression fits a straight line, whereas logistic regression uses a sigmoid curve.</li>
                                <li>Logistic regression is evaluated using classification metrics like accuracy and F1-score, while linear regression uses RMSE or R².</li>
                            </ul>
                        </p>
                    </li>

                    <li>
                        <p class="h2-content">
                            <b>Does logistic regression use the Sigmoid function?</b><br>
                            Yes, logistic regression uses the Sigmoid function to convert any real-valued number into a probability between 0 and 1.
                            This ensures that the output is always between 0 and 1, making it suitable for binary classification.
                    </li>

                    <li>
                        <p class="h2-content">
                            <b>Explain how maximum likelihood is connected to logistic regression?</b><br>
                            Logistic regression does not use the least squares method like linear regression. Instead, it uses 
                            Maximum Likelihood Estimation (MLE) to find the best coefficients (\(\beta\)).<br>
                            MLE maximizes the probability that the predicted class labels match the actual labels in the training data.
                            It calculates the likelihood of the observed data given the model parameters and optimizes those parameters 
                            to make the predictions as accurate as possible.
                        </p>
                    </li>
                </ol>
            </p>

        <!-- Code -->
            <h2>Data Preparation</h2>
            <p class="h2-content">
                <ol>
                    <li>
                        <b><em>Image of before & after cleaning data.</em></b><br>
                        For Logistic Regression Modeling, a subset of data which includes only two label categories was used.
                        The purpose is to classify the lithology of Sandstone Shale and Limestone.<br>
                        The featues are cleaned and normalized using StandardScaler to ensure all features have a mean of 0 
                        and a standard deviation of 1.
                        <div class="image">
                            <img style="height: 250px" src="assets/img/reg/reg_b_a.png" 
                            alt="data-cleaning" class="grid-image" data-description="">
                            <p class="image-caption">Data before and after cleaning and preprocessing process.</p>
                        </div>
                        
                        The dataset has 4,909 samples included 2,523 samples of Sandstone Shale and 2,386 sample of Limestone.
                        <div class="image">
                            <img style="height: 500px" src="assets/img/reg/reg_3d_data.png" 
                            alt="3d-view-data" class="grid-image" data-description="">
                            <p class="image-caption">3D visualization of dataset for Logistic Regression modeling.</p>
                        </div>
                    </li>

                    <li>
                        <b><em>Image of the sample train and test data.</em></b><br>
                        Train - Test splitting is conducted to evaluate the model's performance, in which 80% of the data is used for training and 20% for testing.
                        The visualization of train-test splitting is shown below:
                        <div class="image">
                            <img style="height: 400px" src="assets/img/reg/reg_train_test_split.png" 
                            alt="train-test-split-procedure" class="grid-image" data-description="">
                            <p class="image-caption">Visualization of train-test splitting.</p>
                        </div>
                    </li>

                    <li>
                        <b><em>How the test train split was created?</em></b><br>
                        Train-test splitting is a fundamental step to evaluate a model's performance. 
                        The dataset is divided into a Training Set, which is seen data that provided for model training, and a Testing Set, which acts as unseen data and 
                        uses to assess its accuracy. <br>
                        
                        <div class="image">
                            <img style="height: 250px" src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_train-test-split_0.jpg" 
                            alt="train-test-split-procedure" class="grid-image" data-description="">
                            <p class="image-caption">Train test split procedure. (<a href="https://builtin.com/data-science/train-test-split" target="_blank">Source</a>)</p>
                        </div>

                        A common split ratio is 80% for training and 20% for testing, though this can vary depending on the dataset size and problem complexity. 
                        This helps evaluate how well the model generalizes to unseen data. The split is often done randomly, but in cases like imbalanced data or 
                        time series, special techniques like stratified or time-based splitting are used to maintain data characteristics or order.
                    <li>
                        <b><em>Why it is important to create a disjoint split?</em></b><br>
                        These sets must be disjoint, meaning they should not share any data points, to ensure a fair evaluation. The purpose is evaluate 
                        the model's generalization ability, which is need to predict real-world data.
                        On the other hand, if the model were tested on data it had already seen, it would give an overly optimistic accuracy, failing to reflect the generalization ability.<br>

                    </li>
                </ol>
            </p> 


            
            <!-- Results -->
            <h2>Results</h2>
            <p class="h2-content">
                <b>Linear Regression</b><br>
                The Linear Regression is a regression model that predicts the continuous values. The model's output is a linear combination of the input features.
                Therefore, to use Linear Regression for classification, a threshold of 0.5 is set to classify the predicted values into two classes.
                The plot below shows the prediction of Linear Regression model on test data:
                <div class="image">
                    <img style="height: 350px; width: 550px;" src="assets/img/reg/reg_lr_plot.png" 
                    alt="confusion-matrix-regression" class="grid-image" data-description="">
                    <p class="image-caption">The prediction of Linear Regression model on test data.</p>
                </div>

                It is clear that there is a difference on the distribution of each class and accidentally, the Linear Regression model is able to classify two lithologies.
                This also reason the accuracy of <b>94.5%</b> on test data. The confusion matrix is shown below, where 1 indicates Sandstone Shale and 0 indicates Limestone:
                <div class="image">
                    <img style="height: 450px; width: 550px;" src="assets/img/reg/reg_lr_cm.png" 
                    alt="confusion-matrix-regression" class="grid-image" data-description="">
                    <p class="image-caption">The confusion matrix of the Linear Regression model.</p>
                </div>
            </p>
            <p class="h2-content">
                <b>Logistic Regression</b><br>
                The Logistic Regression was trained on training set to predict the lithologies of Sandstone Shale and Limestone.
                The accuracy on testing set is <b>94.4%</b> and the confusion matrix is shown below:
                <div class="image">
                    <img style="height: 450px; width: 550px;" src="assets/img/reg/reg_cm.png" 
                    alt="confusion-matrix-regression" class="grid-image" data-description="">
                    <p class="image-caption">The confusion matrix of the Logistic Regression model.</p>
                </div>

                <br><br>
                <b>Multinomial Naïve Bayes</b><br>
                The Multinomial Naïve Bayes was trained the same training set and evaluated with defined testing set.
                The accuracy on testing set is <b>92.06%</b> and the confusion matrix is shown below, where 1 indicates 
                Sandstone Shale and 0 indicates Limestone:
                <div class="image">
                    <img style="height: 450px; width: 550px;" src="assets/img/reg/mnb_cm.png" 
                    alt="confusion-matrix-multinomial-nb" class="grid-image" data-description="">
                    <p class="image-caption">The confusion matrix of the Multinomial Naïve Bayes model.</p>
                </div>

                To compare the performance of three models, the comprehensive table of metrics including 
                precision, recall, and f1-score has been constructed. The details are shown below:
                <div class="image">
                    <img style="height: 180px;" src="assets/img/reg/reg_report.png" 
                    alt="confusion-matrix-multinomial-nb" class="grid-image" data-description="">
                    <p class="image-caption">The performance metrics of Linear Regression and Multinomial Naive Bayes models.</p>
                </div>
                It is clear that all three models yield high accuracy, with Logistic Regression and Linear Regression achieving 94.4% and 94.5% accuracy, respectively.
                The Multinomial Naïve Bayes model achieved 92.06% accuracy.
            </p>

        <!-- Conclusions -->
            <h2>Conclusions</h2>
            <p class="h2-content">
                By modeling regression algorithms, it is clear to realize that regression is a fundamental approach in 
                supervised learning used to predict continuous outcomes. It helps capture the relationship between input 
                variables and a numerical target, making it widely applicable in areas like forecasting, risk analysis, 
                and pricing models.<br><br>

                However, in this experiment, the Regression is designed to classify the lithologies of Sandstone Shale and
                Limestone. The Linear Regression is conducted with a threshold of 0.5 to classify the predicted values into two classes.
                While the Logistic Regression is trained to predict the probability of the target class and the Multinomial Naïve Bayes 
                counts the number of occurrences of each feature in each class for prediction.<br><br>
                
                <b><em>Comparison the result of all three models .</em></b><br>
                All approaches yield high accuracy, with Logistic Regression and Linear Regression achieving 94.4% and 94.5% accuracy, respectively.
                The Multinomial Naïve Bayes model achieved 92.06% accuracy.<br><br>
                
                The close performance between Logistic and Linear Regression suggests that the dataset may have a strong linear relationship 
                and is well-suited for models that assume linear decision boundaries. Despite Linear Regression being primarily designed for 
                continuous output, when adapted for classification (e.g., by thresholding), it can still perform competitively in certain binary 
                tasks. Logistic Regression, however, is more appropriate for classification problems due to its probabilistic output and handling 
                of binary outcomes, which also contributes to its robust and interpretable performance.<br><br>

                Meanwhile, the Multinomial Naïve Bayes model, while slightly lower in accuracy, is known for its simplicity and speed, 
                particularly in categorical data. Its assumption of feature independence may limit its performance when 
                features are correlated, which could explain the slight drop in accuracy compared to regression models. 
                <br><br>

                <b><em>Why a specific algorithm is works/does not work for your project?</em></b><br>
                The reason for the high accuracy is the dataset is well-prepared and cleaned, and the features are normalized. The difference in 
                data distribution of two classes is also a reason for the high accuracy.

            </p>
<!-- Deliverables -->
            <section>
                <h2>Deliverables</h2>
                <p class="h2-content">
                    For your reference, all external links are provided below:
                </p>
                <div class="external-links">
                    <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/reg/reg_before.csv" target="_blank" class="item">
                        <img src="assets/icons/table-solid.svg" alt="data" data-description="">
                        Data Before
                    </a>
                    <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/data/reg/reg_after.csv" target="_blank" class="item">
                        <img src="assets/icons/table-solid.svg" alt="data" data-description="">
                        Data After (Cleaned)
                    </a>
                    <a href="https://github.com/tuanna712/MLDSSpring25/blob/main/notebooks/07_regression.ipynb" target="_blank" class="item">
                        <img src="assets/icons/github-brands-solid.svg" alt="github" data-description="">
                        Data Preparation and Modeling Notebook
                    </a>
                </div>
            </section>
        </div>
        <!-- Image Expanded View -->
        <div class="overlay" id="overlay">
            <div>
                <button class="close-btn" id="closeBtn"></button>
            </div>
            <div class="img-disp" id="imgDisp">
                <img id="expandedImage" src="" alt="Expanded Image">
            </div>

            <div class="img-desc" id="imgDesc">
                <p id="imageDescription"></p>
            </div>
        </div>
        <!-- Footer -->
        <footer>
            <div class="footer-container">
                <p>&copy; 2025 Tuan Nguyen.</p>
            </div>
        </footer>
    </body>
</html>